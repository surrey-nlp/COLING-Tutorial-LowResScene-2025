{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06 - BERT Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take you through a practical scenario on how to **finetune** a **pre-trained** language model (BERT) on the task of **Abbreviation Detection** using a subset of the **PLOD dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (2.18.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (4.36.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: spacy in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy-transformers in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (1.3.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy-transformers) (3.7.4)\n",
      "Requirement already satisfied: transformers<4.37.0,>=3.4.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy-transformers) (4.36.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy-transformers) (2.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy-transformers) (2.4.8)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy-transformers) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy-transformers) (1.26.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (3.2.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch>=1.8.0->spacy-transformers) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.21.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers<4.37.0,>=3.4.0->spacy-transformers) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from sympy->torch>=1.8.0->spacy-transformers) (1.3.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (4.66.2)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from transformers[torch]) (0.28.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seqeval in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from seqeval) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary dependencies\n",
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install spacy\n",
    "%pip install torch\n",
    "%pip install spacy-transformers\n",
    "%pip install transformers[torch]\n",
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset\n",
    "\n",
    "To access PLOD, we will use the HuggingFace Datasets library.\n",
    "Hugging Face is a company and an open-source community that primarily focuses on natural language processing (NLP) technologies. \n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Language Model (BERT)\n",
    "\n",
    "They are best known for their development and maintenance of the \"Transformers\" library, which is a popular open-source library for state-of-the-art NLP architectures. We can use it to load our BERT model and its pre-trained weights. They also offer other pipeline compones like model-specific tokenizers.\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset pre-processing\n",
    "** **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start out by separating the 3 subsets of PLOD.\n",
    "\n",
    "*(Train, validation and test data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_dataset = dataset[\"train\"][:200]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tokenize the train set using BERT's tokenizer, which will get our sentences into the form that the model is used to seeing during pre-training. In this case, the authors added two special tokens to their sentences:\n",
    "-   [CLS]: The final embedding of this token is usually treated as the overall representation of the entire input.\n",
    "-   [SEP]: A separator token that is used in task that require of multiple sentences to inform the model about their limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'for', 'this', 'purpose', 'the', 'gothenburg', 'young', 'persons', 'empowerment', 'scale', '(', 'g', '##ype', '##s', ')', 'was', 'developed', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(short_dataset[\"tokens\"], is_split_into_words=True)\n",
    "\n",
    "# Example single sentence example.\n",
    "for token in tokenized_input[\"input_ids\"]:\n",
    "    print(tokenizer.convert_ids_to_tokens(token))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PLOD dataset provides its labels in string form, we have to convert them to class indexes so that they can be understood by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}\n",
    "\n",
    "label_list = []\n",
    "for sample in short_dataset[\"ner_tags\"]:\n",
    "    label_list.append([label_encoding[tag] for tag in sample])\n",
    "\n",
    "val_label_list = []\n",
    "for sample in val_dataset[\"ner_tags\"]:\n",
    "    val_label_list.append([label_encoding[tag] for tag in sample])\n",
    "\n",
    "test_label_list = []\n",
    "for sample in test_dataset[\"ner_tags\"]:\n",
    "    test_label_list.append([label_encoding[tag] for tag in sample])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we added two special tokens to our input sentences, we have to align the labels to account for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(short_dataset, list_name):\n",
    "    tokenized_inputs = tokenizer(short_dataset[\"tokens\"], truncation=True, is_split_into_words=True) ## For some models, you may need to set max_length to approximately 500.\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(list_name):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenize_and_align_labels(short_dataset, label_list)\n",
    "tokenized_val_datasets = tokenize_and_align_labels(val_dataset, val_label_list)\n",
    "tokenized_test_datasets = tokenize_and_align_labels(test_dataset, test_label_list)\n",
    "# print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT's tokenizer returns the dataset in the form of a dictionary of lists (sentences). \n",
    "# we have to convert it into a list of dictionaries for training.\n",
    "def turn_dict_to_list_of_dict(d):\n",
    "    new_list = []\n",
    "\n",
    "    for labels, inputs in zip(d[\"labels\"], d[\"input_ids\"]):\n",
    "        entry = {\"input_ids\": inputs, \"labels\": labels}\n",
    "        new_list.append(entry)\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_train = turn_dict_to_list_of_dict(tokenized_datasets)\n",
    "tokenised_val = turn_dict_to_list_of_dict(tokenized_val_datasets)\n",
    "tokenised_test = turn_dict_to_list_of_dict(tokenized_test_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve training efficiency, we can parallelize it by feeding multiple sentences to BERT at once. Data collators are objects that will form a batch by using a list of dataset elements as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training and metrics\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# Training arguments (feel free to play arround with these values)\n",
    "model_name = \"bert-base-uncased\"\n",
    "epochs = 6\n",
    "batch_size = 4\n",
    "learning_rate = 2e-5\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"BERT-finetuned-NER\",\n",
    "    # evaluation_strategy = \"epoch\", ## Instead of focusing on loss and accuracy, we will focus on the F1 score\n",
    "    evaluation_strategy ='steps',\n",
    "    eval_steps = 7000,\n",
    "    save_total_limit = 3,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.001,\n",
    "    save_steps=35000,\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenised_train,\n",
    "    eval_dataset=tokenised_val,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [05:08<00:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 308.1618, 'train_samples_per_second': 1.947, 'train_steps_per_second': 0.487, 'train_loss': 0.06668521245320638, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.06668521245320638, metrics={'train_runtime': 308.1618, 'train_samples_per_second': 1.947, 'train_steps_per_second': 0.487, 'train_loss': 0.06668521245320638, 'epoch': 6.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 38/39 [00:15<00:00,  2.56it/s]c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\gk00554\\.conda\\envs\\nlp24\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [1, 0, 2, 3, 3, 0] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "100%|██████████| 39/39 [00:15<00:00,  2.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0]': {'precision': 0.5760869565217391,\n",
       "  'recall': 0.7940074906367042,\n",
       "  'f1': 0.6677165354330709,\n",
       "  'number': 267},\n",
       " '0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 1, 0, 0, 0, 0]': {'precision': 0.5187861271676301,\n",
       "  'recall': 0.6697761194029851,\n",
       "  'f1': 0.5846905537459284,\n",
       "  'number': 536},\n",
       " '0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 2, 3, 3, 0, 1, 0, 0, 0, 0, 0]': {'precision': 0.4789473684210526,\n",
       "  'recall': 0.610738255033557,\n",
       "  'f1': 0.5368731563421829,\n",
       "  'number': 149},\n",
       " '1, 0, 2, 3, 3, 0]': {'precision': 0.4523809523809524,\n",
       "  'recall': 0.5891472868217055,\n",
       "  'f1': 0.5117845117845118,\n",
       "  'number': 129},\n",
       " 'overall_precision': 0.5204513399153737,\n",
       " 'overall_recall': 0.6827012025901943,\n",
       " 'overall_f1': 0.5906362545018007,\n",
       " 'overall_accuracy': 0.9043752819124944}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the test data for evaluation in the same format as the training data\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenised_test)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove the predictions for the [CLS] and [SEP] tokens \n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "# Compute multiple metrics on the test restuls\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can observe the F1 score for your fine-tuned model, you should try and change the name of the Transformer model to roberta-base and look at the performance. Further, before you start experimenting with Transformers for the coursework, I would suggest having a look at the [Huggingface MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard). MTEB is a Massive Text Embedding Benchmark which allows you to check how each model is performing on the each subset of this benchmark data. \n",
    "\n",
    "HINT: Is there a sequence/token classification task among the tasks/dataset listed on the benchmark? If you specifically check for performance measures for the token classification, you may be able to get an indication about how it may perform compare to other models on the board. Check out for the rank of the ['SentenceBERT' model](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) called all-mpnet-base-v2, find out where bert-base models rank, and where roberta ranks on this list; see if any other models can help you with the coursework (CW)   \n",
    "\n",
    "However, I request you to look carefully at the model size for roberta-base/bert-base/all-mpnet-base-v2; and find a model from the leaderboard which is similar in terms of size so that you do not get out of memory error during the fine-tuning process. Remember that free tier GPUs on Colab offer only 16GB of GPU RAM, and any fine-tuning experiments for Transformers should be performed with GPU; CPU-based fine-tuning can be (very) slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
